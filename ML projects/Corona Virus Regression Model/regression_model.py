# -*- coding: utf-8 -*-
"""Regression model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10v-LkGn7U1Kp7aIudsZXmiQcJPQ8_Lw5

# **This is the regression model for corona virus dataset**

#The dataset needs to be sliced into smaller dataframes since the size is exceeding RAM
"""

#importing librarries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
sns.set(style="darkgrid")

#generating the dataset
def generate_data(location:str,sample_number=10):
  data=pd.read_csv(location,engine='python')
  head=data.head()
  tail=data.tail()
  sample=data.sample(sample_number)
  description=data.describe()
  columns=data.columns
  info=data.info()
  shape=data.shape
  size=data.size
  return {'data':data,'head':head,'tail':tail,'sample':sample,'description':description,'columns':columns,'info':info,
          'shape':shape,'size':size}

data_set=generate_data('/content/covid_19_data.csv')
data=data_set['data']

data.shape

"""#In this project will be predicting following:

#1)Confirmed cases

#2)Deaths

#3)Recovered patients
"""

data.head()

data.tail()

data.sample(10)

"""#EDA of the dataset

#This is the distribution of each of the feature usig plotly for an interactive plot
"""

def showDistribution(country:str):
  print(country)
  Data=data[data['Country/Region'] == country]
  print('This is the covid data belonging the country you entered')
  display(Data)
  print('These are the distributions of the data ')
  figure, axis = plt.subplots(3, 2,constrained_layout = True)
  axis[0, 0].plot(Data['ObservationDate'],Data['Confirmed'])
  axis[0, 0].set_title("Confirmed cases")
  axis[0, 1].plot(Data['ObservationDate'],Data['Deaths'])
  axis[0, 0].set_title("Deaths")
  axis[1, 0].plot(Data['ObservationDate'],Data['Recovered'])
  axis[0, 0].set_title("Recovered")
  axis[1, 1].plot(Data['Confirmed'],Data['Deaths'])
  axis[0, 0].set_title("Confirmed VS Deaths")
  axis[2, 0].plot(Data['Confirmed'],Data['Recovered'])
  axis[0, 0].set_title("Confirmed VS Recovered")
  plt.subplot_tool()
  plt.show()

showDistribution('Canada')

def newData(country:str):
  Data=data[data['Country/Region'] == country]
  return Data

countryData=newData('Canada')
display(countryData)
#now working on this subset of the main data that belongs to a specific country/region

"""#Now filtering the dataset based on the country you selected

#Animations
"""

#cases
px.scatter(countryData,x='ObservationDate',y='Confirmed',animation_frame='ObservationDate',animation_group='Province/State',
           size='Confirmed', color='Province/State',hover_name='Province/State',
           log_x=True, size_max=55)

#deaths
px.scatter(countryData,x='ObservationDate',y='Deaths',animation_frame='ObservationDate',animation_group='Province/State',
           size='Deaths', color='Province/State',hover_name='Province/State',
           log_x=True, size_max=100000)

#recovered
px.scatter(countryData,x='ObservationDate',y='Recovered',animation_frame='ObservationDate',animation_group='Province/State',
           size='Recovered', color='Province/State',hover_name='Province/State',
           log_x=True, size_max=100000)

#confirmed vs deaths
px.scatter(countryData,x='Confirmed',y='Deaths',animation_frame='ObservationDate',animation_group='Province/State',
           size='Deaths', color='Province/State',hover_name='Province/State',
           log_x=True, size_max=100000,range_x=[0,10e6], range_y=[0,10e6])

#confirmed vs recovered
px.scatter(countryData,x='Confirmed',y='Recovered',animation_frame='ObservationDate',animation_group='Province/State',
           size='Recovered', color='Province/State',hover_name='Province/State',
           log_x=True, size_max=100000,range_x=[0,10e6], range_y=[0,10e6])

def get_type(data):
  numeric=[]
  categorical=[]
  for col in data.columns:
    if data[f'{col}'].dtypes == 'object':
      categorical.append(col)
    else:
      numeric.append(col)
  return {'numeric':numeric,'categorical':categorical}

# check for null values and deal with them
# this function will take the type of process as well for both numeric and categorical data
def treat_null_values(data,numeric_type:str='mean'):
  types=get_type(data)
  numeric=types['numeric']
  categorical=types['categorical']
  if numeric_type == 'mean':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].mean())
  elif numeric_type == 'mode':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].mode())
  elif numeric_type == 'median':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].median())
  elif numeric_type == 'frequent':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].nunique[0])
  elif numeric_type == 'drop':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].dropnna(inplace=True)
  elif numeric_type == 'predictive_modeling':
    pass # create a seprate function for this
  elif numeric_type == 'impute':
    pass # create a seprate function for this as well
  
  for col in categorical:
    most_frequent_category=data[f'{col}'].mode()[0]
    data[f'{col}'].fillna(most_frequent_category,inplace=True)
  return data

  

def predictive_modeling():
  pass #do a detailed study as disadvantages for this model usually outweights advantages
def multiple_imputation():
  from fancyimpute import IterativeImputer as MICE
  data= pd.DataFrame(MICE().fit_transform(data))
  return data

def encode_data(data,multiclass:str='One_hot',binary_class:str='Label'): #this function takes three args one is the data 2nd is the type of encoding for multiclass data and third is the encoding for binary class data
  categorical=get_type(data)['categorical']
  multivariate=[]
  bivariate=[]
  for col in categorical:
    if data[f'{col}'].nunique()>2:
      multivariate.append(col)
    else:
      bivariate.append(col)
  
  if multiclass == 'One_hot':
    for col in multivariate:
      data=encode_and_bind(data,col)
  if binary_class == 'Label':
    for col in bivariate:
      data=label_encode(data,col)
  # add other sorting techniques as well in here
  return data

def encode_and_bind(original_dataframe, feature_to_encode):
    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])
    res = pd.concat([original_dataframe, dummies], axis=1)
    res.pop(feature_to_encode)
    return(res)

def label_encode(data,col):
  from sklearn.preprocessing import LabelEncoder
  encoder=LabelEncoder()
  data[col]=encoder.fit_transform(data[col])
  return data
  
# this function needs to be modified therefore add differenr sort of encoding techniques as well in this cll

def treat_outliers(data,feature:str,algo:str='IQR',z_threshold:int=3,add_feature=None,remove:bool=False,scatter:bool=False,feature_x=None,feature_y=None): # this function takes two arguments one is data and 2ns is the algorithm
  pass
  if scatter == True:
    px.scatter(data_px,x=feature_x,y=feature_y,hover_name='Country')
  else:
    if algo == 'IQR':
      outlier_index=Inter_quantile_range(data,feature)
    if algo == 'EEA': # elliptic envelope algo
      outlier_index=Elliptice_envelope_algo(data,feature,add_feature)
    if algo == 'ISF': # Isolate forest algo
      outlier_index=Isolate_forest_algo(data,feature)
    if algo == 'One_classSVM': # one class svm
      outlier_index=One_class_SVM(data,feature)
    if algo == 'LOF':
      outlier_index=Local_factor_outlier(data,feature)
    if algo == 'Z_score':
      outlier_index=Z_score_algo(data,feature,z_threshold)
    print(outlier_index)
    if remove == True:
      data=remove_outliers(outlier_index,data)
      return data
    else:
      return outlier_index
    


# in all these function display the index of outliers

def Local_factor_outlier(data,feature):
  import numpy as np 
  from sklearn.neighbors import LocalOutlierFactor
  X=data[[feature,data.columns.values[-1]]]
  lof = LocalOutlierFactor(n_neighbors=20, algorithm='auto',
                         metric='minkowski', contamination=0.04,
                         novelty=False, n_jobs=-1)
  pred = lof.fit_predict(X)
  outlier_index = np.where(pred==-1)
  return outlier_index

def One_class_SVM(data,feature): #this returns novelty I am guessing so fuck this method
  import numpy as np
  from sklearn.svm import OneClassSVM
  X=data[[feature,data.columns.values[-1]]]
  one_class_svm = OneClassSVM(kernel='rbf', degree=3, gamma='scale')
  new_data = np.array([[-4, 8.5]])# change these values as per your dataset
  one_class_svm.fit(X)
  pred = one_class_svm.predict(new_data)
  outlier_index = np.where(pred==-1)
  return outlier_index

def Isolate_forest_algo(data,feature):
  import numpy as np
  from sklearn.ensemble import IsolationForest
  from sklearn.decomposition import PCA
  from sklearn.preprocessing import StandardScaler
  X=data[[feature,data.columns.values[-1]]]
   # Returns 1 of inliers, -1 for outliers
  iforest = IsolationForest(n_estimators=100, max_samples='auto', 
                          contamination=0.05, max_features=1.0, 
                          bootstrap=False, n_jobs=-1, random_state=1)
  pred = iforest.fit_predict(X)
 # Extract outliers
  outlier_index = np.where(pred==-1)
  return outlier_index

def Elliptice_envelope_algo(data,feature,add_feature):
  import numpy as np
  from sklearn.covariance import EllipticEnvelope
  elpenv = EllipticEnvelope(contamination=0.025, 
                          random_state=1)
  X=data[[feature,add_feature]]
# Returns 1 of inliers, -1 for outliers
  pred = elpenv.fit_predict(X)

# Extract outliers
  outlier_index = np.where(pred==-1)
  return outlier_index

def Inter_quantile_range(data,feature):
  Q1 = np.percentile(data[f'{feature}'], 25, interpolation = 'midpoint') 
  Q2 = np.percentile(data[f'{feature}'], 50, interpolation = 'midpoint') 
  Q3 = np.percentile(data[f'{feature}'], 75, interpolation = 'midpoint') 
  IQR = Q3 - Q1 
  low_lim = Q1 - 1.5 * IQR
  up_lim = Q3 + 1.5 * IQR
  outlier_index=[]
  for val in data[f'{feature}']:
    if val > up_lim or val <low_lim:
      outlier_index.append(data.index[data[f'{feature}'] == val].values[0])
  return outlier_index
         
def Z_score_algo(data,feature,z_threshold):
  mean = np.mean(data[f'{feature}'])
  std = np.std(data[f'{feature}'])
  print('mean of the dataset is', mean)
  print('std. deviation is', std)
  outlier_index = []
  for val in data[f'{feature}']:
    z = (val-mean)/std
    if z > z_threshold:
      outlier_index.append(data.index[data[f'{feature}'] == val].values[0])
  return outlier_index
  


# this function will remove the outliers if removal is allowed
def remove_outliers(index,data):
  for row in index:
    data.drop([row])
    print(f'dropped value at index {row}')
  return data

      

# if remove is true remove outliers else if it is false just return the index of outliers
# this will function will be returning two datasets the original one and one in which all the outliers are removed

# this function will perform all the data preprocessing steps
# incase you want to use non default algos call these function seprately
def preprocess_data(data,feature:str):
  data=treat_null_values(data)
  data=encode_data(data)
  last_column = data[feature]
  data.drop(feature, inplace=True, axis=1)
  #dropping country since it will be same for all the entries
  data.drop('Country/Region',inplace=True,axis=1)
  data.insert(data.shape[1],feature,last_column)
  
  return data

# call outliers and balance function independentlu whenever you like and PS Fuck Roopa

#generating three new dataset for following features:
'''
confirmed cases
Deaths
Recovered
'''
print(countryData.columns)

#THESE ARE THE THREE NEW DATASETS BASED ON THE THREE FEATURES THIS MODEL WILL BE PREDICTING
Cases=preprocess_data(countryData,'Confirmed')
Deaths=preprocess_data(countryData,'Deaths')
Recovered=preprocess_data(countryData,'Recovered')

#CORELLATION BETWEEN DIFFERENT FEATURES
def create_heat_map(data):
  #correalation between varaibles
  plt.figure(figsize=(15, 12))
  heatmap = sns.heatmap(data.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')
  heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
  # save heatmap as .png file
  # dpi - sets the resolution of the saved image in dots/inches
  # bbox_inches - when set to 'tight' - does not allow the labels to be cropped
  plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')# this will will also return two dataset one being the original one and second one will be the one in

create_heat_map(countryData)

"""#The greater corellation value between deaths and confimed cases then compared to confiremd cases and recovery should be a cause of concen.

#This is the EDA of teh new preprocessed data
"""

Cases.describe()

Deaths.describe()

Recovered.describe()

#Checking the new data for outliers before scaling it
#A)Visualizing it using the scatter plot
px.scatter(Cases,y='Confirmed')

px.scatter(Cases,y='Deaths')

px.scatter(Cases,y='Recovered')

#using statistical methods for finding outliers confirmed cases
outliers1=treat_outliers(Cases,feature='Confirmed')

outliers2=treat_outliers(Cases,feature='Deaths')

outliers3=treat_outliers(Cases,feature='Recovered')

print(f'Number of outliers {len(outliers1)}')
print(f'Number of outliers {len(outliers2)}')
print(f'Number of outliers {len(outliers3)}')
#since there are uneven distriubtion of number of cases in different countries 
#therefore there will be outliers present in the dataset.

"""#Splitting the data into test and train data"""

def split_data(data,dependent_feature:str=data.columns[-1],test_data_percentage=0.25,random_state=42):
  last_column = data[dependent_feature]
  data.drop(dependent_feature, inplace=True, axis=1)
  data.insert(data.shape[1],dependent_feature,last_column)
  x=data.iloc[:,:-1]
  y=data.iloc[:,-1]
  from sklearn.model_selection import train_test_split
  x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=test_data_percentage,random_state=random_state)
# this function takes three args one being the data and other the percent of test data and the random_state
# this function will return 4 args x_train,x_test,y_train,y_test
# add shape and size as well for all the params
# in this format 
  return{'x_train':{"data":x_train,"shape":x_train.shape,"size":x_train.size},
       'x_test':{"data":x_test,"shape":x_test.shape,"size":x_test.size},
       'y_train':{"data":y_train,"shape":y_train.shape,"size":y_train.size},
       'y_test':{"data":y_test,"shape":y_test.shape,"size":y_test.size}}

#cases data
cases_split=split_data(Cases)
#deaths data
deaths_split=split_data(Deaths)
#recovered data
recovered_split=split_data(Recovered)

#cases
x_trainCases=cases_split['x_train']['data']
x_testCases=cases_split['x_test']['data']
y_trainCases=cases_split['y_train']['data']
y_testCases=cases_split['y_test']['data']
#deaths
x_trainDeaths=deaths_split['x_train']['data']
x_testDeaths=deaths_split['x_test']['data']
y_trainDeaths=deaths_split['y_train']['data']
y_testDeaths=deaths_split['y_test']['data']
#recovered
x_trainRecovered=recovered_split['x_train']['data']
x_testRecovered=recovered_split['x_test']['data']
y_trainRecovered=recovered_split['y_train']['data']
y_testRecovered=recovered_split['y_test']['data']

display(x_testCases)

display(x_trainCases)

display(y_testCases)

display(y_trainCases)

def scale_data(x_train,x_test,type:str='Standard'):
  if type == 'Standard':
    from sklearn.preprocessing import StandardScaler
    sc=StandardScaler()
    x_train=sc.fit_transform(x_train)
    x_test=sc.transform(x_test)
  elif type == 'Normalize':
    pass
  
  return {'x_train':x_train,'x_test':x_test}
 # this function takes three args x_train,x_test and the type of scaling that should be applied

#scaling the data
casesScaled=scale_data(x_trainCases,x_testCases)
x_train_scaledCase=casesScaled['x_train']
x_test_scaledCase=casesScaled['x_test']

deathsScaled=scale_data(x_trainDeaths,x_testDeaths)
x_train_scaledDeaths=casesScaled['x_train']
x_test_scaledDeaths=casesScaled['x_test']

recoveredScaled=scale_data(x_trainRecovered,x_testRecovered)
x_train_scaledRecovered=casesScaled['x_train']
x_test_scaledRecovered=casesScaled['x_test']

"""#Processing without scaling quki system ke aukaat ke bahar hai ye"""

# seprate function for quantile,weighted and bootstrapping regression
def prepare_models(x_train,y_train,model:str='all'):
  models={}
  if model == 'all':
    Linear_regressor=Linear_regression(x_train,y_train)
    SVR_regressor=Support_vector_regression(x_train,y_train)
    Decision_tree_regressor=Decision_tree_regression(x_train,y_train)
    Random_forest_regressor=Random_forest_regression(x_train,y_train)
    XGB_regressor=XGB_regression(x_train,y_train)
    Ridge_regressor=Ridge_L2_regression(x_train,y_train)
    Kernel_ridge_regressor=Kernel_ridge_regression(x_train,y_train)
    Lasso_regressor=Lasso_L1_regression(x_train,y_train)
    Elastic_net_regressor=Elastic_net_regression(x_train,y_train)
    Hubber_regressor=Hubber_regression(x_train,y_train)
    Extra_tree_regressor=Extra_tree_regression(x_train,y_train)
    Bayseian_ridge_regressor=Bayseian_ridge_regression(x_train,y_train)
    Light_gradient_boosting_regressor=Light_gradient_boosting_regression(x_train,y_train)
    gradient_boosting_regressor=gradient_boosting_regression(x_train,y_train)
    adaboost_regressor=adaboost_regression(x_train,y_train)
    models={'SVR':SVR_regressor,'DT':Decision_tree_regressor,
            'RF':Random_forest_regressor,'XGB':XGB_regressor,'Ridge':Ridge_regressor,
            'Kernel_ridge':Kernel_ridge_regressor,'Lasso':Lasso_regressor,
            'Elastic':Elastic_net_regressor,'Hubber':Hubber_regressor,'ET':Extra_tree_regressor,
            'Bayseian':Bayseian_ridge_regressor,'Light':Light_gradient_boosting_regressor,
            'gradient':gradient_boosting_regressor,'adaboost':adaboost_regressor,'LR':Linear_regressor}
  elif model == 'LR':
    Linear_regressor=Linear_regression(x_train,y_train)
    models={'LR':Linear_regressor}
  elif model == 'SVR':
    SVR_regressor=Support_vector_regression(x_train,y_train)
    models={'SVR':Support_vector_regressor}
  elif model == 'DT':
    Decision_tree_regressor=Decision_tree_regression(x_train,y_train)
    models={'DT':Decision_tree_regressor}
  elif model == 'RF':
    Random_forest_regressor=Random_forest_regression(x_train,y_train)
    models = {'RF':Random_forest_regressor}
  elif model == 'XGB':
    XGB_regressor=XGB_regression(x_train,y_train)
    models={'XGB':XGB_regressor}
  elif model == 'Ridge':
    Ridge_regressor=Ridge_L2_regression(x_train,y_train)
    models={'Ridge':Ridge_regressor}
  elif model == 'Kernel_ridge':
    Kernel_ridge_regressor=Kernel_ridge_regression(x_train,y_train)
    models={'Kernel_ridge':Kernel_ridge_regressor}
  elif model == 'Lasso':
    Lasso_regressor=Lasso_L1_regression(x_train,y_train)
    models={'Lasso':Lasso_regressor}
  elif model == 'Elastic':
    Elastic_net_regressor=Elastic_net_regression(x_train,y_train)
    models={'Elastic':Elastic_net_regressor}
  elif model == 'Hubber':
    Hubber_regressor=Hubber_regression(x_train,y_train)
    models={'Hubber':Hubber_regressor}
  elif model == 'ET':
    Extra_tree_regressor=Extra_tree_regression(x_train,y_train)
    models={'ET':Extra_tree_regressor}
  elif model == 'Bayseian':
    Bayseian_ridge_regressor=Bayseian_ridge_regression(x_train,y_train)
    models={'Bayseian':Bayseian_ridge_regressor}
  elif model == 'Light':
    Light_gradient_boosting_regressor=Light_gradient_boosting_regression(x_train,y_train)
    models={'Light':Light_gradient_boosting_regressor}
  elif model == 'gradient':
    gradient_boosting_regressor=gradient_boosting_regression(x_train,y_train)
    models={'gradient':gradient_boosting_regressor}
  elif model == 'adaboost':
    adaboost_regressor=adaboost_regression(x_train,y_train)
    models={'adaboost':adaboost_regressor}

  return models




def Linear_regression(x_train,y_train):
  from sklearn.linear_model import LinearRegression
  regressor=LinearRegression()
  regressor.fit(x_train,y_train)
  return regressor

def Support_vector_regression(x_train,y_train):
  from sklearn.svm import SVR
  regressor=SVR(kernel='rbf')
  regressor.fit(x_train,y_train)
  return regressor

def Decision_tree_regression(x_train,y_train):
  from sklearn.tree import DecisionTreeRegressor
  regressor=DecisionTreeRegressor() 
  regressor.fit(x_train,y_train)
  return regressor

def Random_forest_regression(x_train,y_train):
  from sklearn.ensemble import RandomForestRegressor
  regressor=RandomForestRegressor(n_estimators=100)
  regressor.fit(x_train,y_train)
  return regressor

def XGB_regression(x_train,y_train):
  from xgboost import XGBRegressor
  regressor = XGBRegressor()#use this with jupyter notebool colab mei chod hai khuch gpu ki
  regressor.fit(x_train, y_train)
  return regressor

def Ridge_L2_regression(x_train,y_train):
  from sklearn.linear_model import Ridge
  regressor=Ridge(alpha=1.0)#alpha is the regularization parametre
  regressor.fit(x_train,y_train)
  return regressor

def Kernel_ridge_regression(x_train,y_train):
  from sklearn.kernel_ridge import KernelRidge
  regressor=KernelRidge(alpha=1.0)#alpha here is the regularization parameter
  regressor.fit(x_train,y_train)
  return regressor

def Lasso_L1_regression(x_train,y_train):
  from sklearn.linear_model import Lasso
  regressor=Lasso(alpha=0.1)#alpha here is the regularization parametre
  regressor.fit(x_train,y_train)
  return regressor

def Elastic_net_regression(x_train,y_train):
  from sklearn.linear_model import ElasticNet
  regressor=ElasticNet(random_state=42)
  regressor.fit(x_train,y_train)
  return regressor

def Hubber_regression(x_train,y_train):
  from sklearn.linear_model import HuberRegressor
  regressor=HuberRegressor(max_iter=100000)
  regressor.fit(x_train,y_train)
  return regressor

# def gamma_regressor(x_train,y_train):
#   pass
# def tweddie_regressor(x_train,y_train):
#   pass
def Extra_tree_regression(x_train,y_train):
  from sklearn.ensemble import ExtraTreesRegressor
  regressor=ExtraTreesRegressor(n_estimators=100,random_state=42)
  regressor.fit(x_train,y_train)
  return regressor
def Bayseian_ridge_regression(x_train,y_train):
  from sklearn.linear_model import BayesianRidge
  regressor=BayesianRidge()
  regressor.fit(x_train,y_train)
  return regressor

def Light_gradient_boosting_regression(x_train,y_train):
  from lightgbm import LGBMRegressor
  regressor=LGBMRegressor()
  regressor.fit(x_train,y_train)
  return regressor

def gradient_boosting_regression(x_train,y_train):
  from sklearn.ensemble import GradientBoostingRegressor
  regressor=GradientBoostingRegressor(random_state=42)
  regressor.fit(x_train,y_train)
  return regressor

def adaboost_regression(x_train,y_train):
  from sklearn.ensemble import AdaBoostRegressor
  regressor=AdaBoostRegressor(random_state=42)
  regressor.fit(x_train,y_train)
  return regressor


def least_angle_regession(x_train,y_train):
  from sklearn import linear_model
  regressor = linear_model.LassoLars(alpha=.1)
  regressor.fit(x_train,y_train)
  return regressor

def orthogonal_matching_pursuit(x_train,y_train):
  from sklearn.linear_model import orthogonal_matching_pursuit
  regressor = orthogonal_matching_pursuit()
  regressor.fit(x_train,y_train)
  return regressor

def automatic_relevance_determination(x_train,y_train):
  from sklearn.linear_model import ARDRegression
  regressor = ARDRegression()
  regressor.fit(x_train,y_train)
  return regressor

def passive_aggressive_regressor(x_train,y_train):
  from sklearn.linear_model import passive_aggressive_regressor
  regressor = passive_aggressive_regressor()
  regressor.fit(x_train,y_train)
  return regressor

def random_sample_consensus(x_train,y_train):
  from sklearn.linear_model import RANSACRegressor
  regressor = RANSACRegressor()
  regressor.fit(x_train,y_train)
  return regressor

def thielsen_regressor(x_train,y_train):
  from sklearn.linear_model import TheilSenRegressor
  regressor = thielsen_regressor()
  regressor.fit(x_train,y_train)
  return regressor

def k_neighbours_regression(x_train,y_train):
  from sklearn.neighbors import KNeighborsRegressor
  regressor = KNeighborsRegressor(n_neighbors=2)
  regressor.fit(x_train, y_trani)
  return regressor

def MLP_regression(x_train,y_train):
  from sklearn.neural_network import MLPRegressor
  regressor = MLPRegressor(random_state=1, max_iter=500)
  regressor.fit(x_train, y_train)
  return regressor

def catboost_regression(x_train,y_train):
  pass
  #it has a seprate module so study the module itself

def make_predictions(models):
  predictions={}
  for model in models:
    print(model)
    y_pred=models[model].predict(x_test)
    predictions[f'y_pred_{model}']=y_pred
  return predictions

def concatenate_test_pred(predictions,y_test):
  compare_predictions={}
  y_test=y_test.to_numpy()
  for prediction in predictions:
    y_pred=predictions[prediction]
    res=np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)
    compare_predictions[f'{prediction}_concatenated']=res
  return compare_predictions
  #this function will return  a dictionary containing the concatenated list of y_test and y_pred for each model

def visualize_models(y_test,predictions):
  a = 5  # number of rows
  b = 5  # number of columns 
  c = 1  # initialize plot counter
  sns.set(rc={"figure.dpi":300, 'savefig.dpi':300})
  fig = plt.figure(figsize=(14,22))
  for prediction in predictions:
    plt.subplot(a, b, c)
    plt.title(f'{prediction}')
    ax1=sns.distplot(y_test,hist=False,color="r",label="actual value")
    sns.distplot(predictions[prediction],hist=False,color="b",label="predicted values",ax=ax1)
    c=c+1
    if c > 12:
      break


#models here is a dict of models using which models will be evaluated

def r2_score_models(models,x_train,y_train):
  from sklearn.model_selection import cross_val_score
  r2_score={}
  for model in models:
    accuracies = cross_val_score(estimator = models[model], X = x_train, y = y_train, cv = 10)
    mean_accuracy = accuracies.mean()
    r2_score[f'{model}_accuracy']={'accuracies':accuracies,'mean_accuracy':mean_accuracy}
  return r2_score


# in this function prepare a dictionary for all the models and the dict will include the mean r2 and the r2 score list

#creting a dataframe
# for score in r2_score:
#   column=score
#   accuracies=r2_score[score]['accuracies']
#   mean_accuracy=r2_score[score]['mean_accuracy']
#   print(f'{column} \t Accuracies:{accuracies} \t mean_accuracy:{mean_accuracy} ')
def prepapreDataframe(r2_score):
  index=[]
  for i in range(0,len(r2_score)):
    index.append(i)
  columns=['model_name','accuracies','mean_accuracy']
  data=[]
  for col in r2_score:
    model_name=col
    accuracies=r2_score[col]['accuracies']
    mean_accuracy=r2_score[col]['mean_accuracy']
    data.append([model_name,accuracies,mean_accuracy])
  r2_data=pd.DataFrame(data=data,index=index,columns=columns)
  r2_data=r2_data.sort_values(by=["mean_accuracy"],ascending=False) 
  return r2_data

def model_evaluation(models,x_train,y_train,r2_score):
  # metrics to be evaluated MAE	MSE	RMSE	Log error R2	RMSLE	MAPE
  test_model=models['ET']
  scoring=['neg_median_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error',
           'r2']
  
  index=[]
  for i in range(0,len(r2_score)):
    index.append(i)
  columns=['Model','MAE','MSE','RMSE','R2']
  
  from sklearn.model_selection import cross_val_score
  data_score=[]
  for model in models:
    data_premature=[]
    data_premature.append(model)
    for score in scoring:
      accuracies=cross_val_score(estimator=models[model], X = x_train, y = y_train, cv = 10,scoring=score,verbose=20,n_jobs=-1)
      data_premature.append(accuracies.mean())
    data_score.append(data_premature)
  score_data=pd.DataFrame(data=data_score,index=index,columns=columns)
  return score_data

"""#This model is for predicting the number of confirmed cases in a specific country"""

caseModels=prepare_models(x_train_scaledCase,y_trainCases)

casePredictions=make_predictions(caseModels)

case_compared_predictions=concatenate_test_pred(casePredictions,y_testCases)
display(case_compared_predictions)

visualize_models(y_testCases,casePredictions)

r2_scoreCases=r2_score_models(caseModels,x_train_scaledCase,y_trainCases)

r2_scoreCase_tabular=prepapreDataframe(r2_scoreCases)

score_dataCase=model_evaluation(caseModels,x_train_scaledCase,y_trainCases,r2_scoreCases )
score_dataCase=score_dataCase.sort_values(by=["R2"],ascending=False) 
display(score_dataCase)

"""#These are the top 5 models for predicting the number of confirmed cases

#1)Extra tree regression model
#2)Random forest regression
#3)Decisin tree regression
#4)Gradient boosting regression
#5)XGB Regression
"""

#hyperrtuning extra tree regression model
hpyerTune1=hypertune_model('ET')

#hyperrtuning extra tree regression model
hpyerTune2=hypertune_model('RF')

#hyperrtuning extra tree regression model
hpyerTune3=hypertune_model('DT')

#hyperrtuning extra tree regression model
hpyerTune4=hypertune_model('gradient')

#hyperrtuning extra tree regression model
hpyerTune5=hypertune_model('XGB')

"""#This model is for predicting the number of deaths in a specific country"""

#repeat similar steps as above in here

"""#This model is for predicting the number of recoveries in a specific country"""

#repeat similar steps as above in here

def hypertune_model(model_name:str):
  if model_name == 'DT':
    param_grid=DT_param_grid()
  elif model_name == 'RF':
    param_grid=RF_param_grid()
  elif model_name == 'SVR':
    param_grid=SVR_param_grid()
  elif model_name == 'XGB':
    param_grid=XGB_param_grid()
  elif model_name == 'Ridge':
    param_grid=Ridge_param_grid()
  elif model_name == 'Lasso':
    param_grid=Lasso_param_grid()
  elif model_name == 'Elastic':
    param_grid=Elastic_param_grid()
  elif model_name == 'Hubber':
    param_grid=Hubber_param_grid()
  elif model_name == 'ET':
    param_grid=ET_param_grid()
  elif model_name == 'Bayseian':
    param_grid=Bayseian_param_grid()
  elif model_name == 'Light':
    param_grid=Light_param_grid()
  elif model_name == 'gradient':
    param_grid=gradient_param_grid()
  elif model_name == 'adaboost':
    param_grid=adaboost_param_grid()
  from sklearn.model_selection import GridSearchCV
  grid=GridSearchCV(estimator=models[model_name],cv=10,verbose=20,n_jobs=-1,param_grid=param_grid,scoring='r2')
  grid.fit(x_train,y_train)
  # Checking the score for all parameters
  print("Grid scores on training set:")
  means = grid.cv_results_['mean_test_score']
  stds = grid.cv_results_['std_test_score']
  for mean, std, params in zip(means, stds, grid.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r"% (mean, std * 2, params))

def DT_param_grid():
  max_depth=[]
  for i in range(0,400):
    max_depth.append(i)
    min_samples_split = np.linspace(0.1, 1.0, 10, endpoint=True)
    min_samples_leaf = np.linspace(0.1, 0.5, 5, endpoint=True)
    max_features = list(range(1,x_train.shape[1]))
    criterion=['mse','mae','poison','friedman_mse']
  parameters={'criterion':criterion,'max_features':max_features,'min_samples_leaf':min_samples_leaf,'min_samples_split':min_samples_split}
  return parameters

def RF_param_grid():
  max_depth=[]
  for i in range(0,400):
    max_depth.append(i)
    min_samples_split = np.linspace(0.1, 1.0, 10, endpoint=True)
    min_samples_leaf = np.linspace(0.1, 0.5, 5, endpoint=True)
    max_features = list(range(1,x_train.shape[1]))
    criterion=['mse','mae','poison','friedman_mse']
    n_estimators=np.linspace(1,100,100,endpoint=True)
    bootstrap=[True,False]
    oob_score=[True,False]
    warm_state=[True,False]
  parameters={'criterion':criterion,'max_features':max_features,'min_samples_leaf':min_samples_leaf,'min_samples_split':min_samples_split,
            'n_estimators':n_estimators,'bootstrap':bootstrap,'oob_score':oob_score,'warm_state':warm_state,'verbos':20}
  return parameters

def SVR_param_grid():
  parameters={'kernel':['rbf','poly','sigmoid'],
            'C': [1.1, 5.4, 170, 1001],
            'epsilon': [0.0003, 0.007, 0.0109, 0.019, 0.14, 0.05, 8, 0.2, 3, 2, 7],
            'gamma': [0.7001, 0.008, 0.001, 3.1, 1, 1.3, 5]}
  return parameters

def XGB_param_grid():
  
  params={'booster':['gblinear','gbtree','dart'],'gamma':[1e-2,1e-1,0.2,0.4,0.6,0.8,1.0,2.0,4.0,6.0,8.0,10.0,15.0,20.0],
           'importance_type':['gain','cover'],'leaning_rate':[1e-3,1e-2,1e-1,0.2,0.3,0.4,0.5,0.9,1.0],
           'max_depth':[1,2,3,4,5,6,7,8,9,10],'min_child_weight':[1,2,4,5,3,6,7,8,9,10],'n_estimators':[100,200,300,400,500],
           'reg_alpha':[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],'reg_lambda':[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0]}
  
  return params  

def Ridge_param_grid():
  parameters=[{'alpha':[1.0,10.0,100.0,1000.0],'solver':['auto','svd','chlosky','lsqr','sparse-cg','sag','saga'],'tol':[1e-4,1e-5,1e-3,1e-2,1e-1,0]}]
  return parameters

def Lasso_param_grid():
  parameters=[{'alpha':[1.0,10.0,100.0,1000.0],'positive':[False,True],'precompute':[True,False],'warm_start':[True,False],'tol':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,0],'selection':['cyclic','random']}]
  return parameters

def Elastic_param_grid():
  parameters=[{'alpha':[1.0,10.0,100.0,1000.0],'positive':[False,True],'precompute':[True,False],'warm_start':[True,False],'tol':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,0],'selection':['cyclic','random'],'l1_ratio':[0.0,0.05,0.1,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95,1.0]}]
  return parameters
  
def Hubber_param_grid():
  parameters={'alpha':[1e-4,1e-5,1e-3,1e-2,1e-1,0],'epsilon':[1.0,1.05,1.10,1.15,1.20,1.25,1.30,1.35,1.40,1.45,1.50
                                                            ,1.55,1.60,1.65,1.70,1.75,1.80,1.85,1.90,1.95,2.0],
            'tol':[1e-05,1e-04,1e-03,1e-02,1e-01,0],
            'warm_start':[True,False]}
  return parameters

 # study these models first before creating the paramgird 
def ET_param_grid():
  max_depth=[]
  for i in range(0,400):
    max_depth.append(i)
    min_samples_split = np.linspace(0.1, 1.0, 10, endpoint=True)
    min_samples_leaf = np.linspace(0.1, 0.5, 5, endpoint=True)
    max_features = list(range(1,x_train.shape[1]))
    criterion=['mse','mae','poison','friedman_mse']
    n_estimators=np.linspace(1,100,100,endpoint=True)
    bootstrap=[True,False]
    oob_score=[True,False]
    warm_start=[True,False]
    parameters={'criterion':criterion,'max_features':max_features,'min_samples_leaf':min_samples_leaf,'min_samples_split':min_samples_split,
            'n_estimators':n_estimators,'bootstrap':bootstrap,'oob_score':oob_score,'warm_start':warm_start}
    return parameters
def Bayseian_param_grid():
  n_iter=[300,400,500]
  tol=[1e-3,1e-2,1e-1,1e-4]
  compute_score=[True,False]
  normalize=[True,False]
  verbose=20
  parameters={'n_iter':n_iter,'tol':tol,'compute_score':compute_score,'normalize':normalize}
  return parameters

def Light_param_grid():
  boosting_type=['gbdt','rf','dart','goss']
  learning_rate=[1e-3,1e-2,1e-1,0.2,0.3,0.4,0.5,0.9,1.0]
  max_depth=[1,2,3,4,5,6,7,8,9,10]
  min_child_weight=[1,2,4,5,3,6,7,8,9,10]
  n_estimators=[100,200,300,400,500]
  reg_alpha=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
  reg_lambda=[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0]
  silent=[True,False]
  objective=['regression','regression_l1','regression_hubber','regression_fair','regression_poisson','resgression_mape','regression_gamma',
           'regression_tweedie','regression_mse','regression_rmse']
  parameters={'boosting_type':boosting_type,'learning_rate':learning_rate,'max_depth':max_depth,
            'min_child_weight':min_child_weight,'n_estimators':n_estimators,'reg-alpha':reg_alpha,
            'reg_lambda':reg_lambda,
            'silent':silent,'objective':objective}
  return parameters
def gradient_param_grid():
  learning_rate=[1e-3,1e-2,1e-1,0.2,0.3,0.4,0.5,0.9,1.0]
  max_depth=[1,2,3,4,5,6,7,8,9,10]
  alpha=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
  n_estimators=[100,200,300,400,500]
  tol=[1e-3,1e-2,1e-1,1e-4]
  parameters={'learning_rate':learning_rate,'max_depth':max_depth,'alpha':alpha,'n_estimators':n_estimators,'tol':tol,
            'warm_start':[True,False]}
  return parameters
def adaboost_param_grid():
  n_estimators=[50,100,200,300,400,500]
  learning_rate=[1e-3,1e-2,1e-1,0.2,0.3,0.4,0.5,0.9,1.0]
  loss=['linear', 'square', 'exponential']
  parameters={'n_estimators':n_estimators,'learning_rate':learning_rate,
            'loss':loss}
  return parameters

#this is the hypertuned model that will be used for predicting the confirmed cases
 #hypertune all the top 5 models one by one and then compare there performance or you can just 
 #hypertune the extra tree regression model.

#while testing the trained and tuned model scale the input data first before
#testing the model.