# -*- coding: utf-8 -*-
"""Clustering model .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zb520qQgVN3Orh_ZBwkL4WtliB1w1p7D

# **This is the clustering model for credit card dataset**
"""

#importing libraries'
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
sns.set(style="darkgrid")

#importing the dataset
def generate_data(location:str,sample_number=10):
  data=pd.read_csv(location,engine='python')
  head=data.head()
  tail=data.tail()
  sample=data.sample(sample_number)
  description=data.describe()
  columns=data.columns
  info=data.info()
  shape=data.shape
  size=data.size
  return {'data':data,'head':head,'tail':tail,'sample':sample,'description':description,'columns':columns,'info':info,
          'shape':shape,'size':size}

data=generate_data('/content/CC GENERAL.csv')['data']

data.head()

data.tail()

data.sample(13)

data.describe()

#correlation
#analyzing the datasets via a heat map to tudy the correlation between the features
def create_heat_map(data):
  #correalation between varaibles
  plt.figure(figsize=(18, 15))
  heatmap = sns.heatmap(data.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')
  heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);
  # save heatmap as .png file
  # dpi - sets the resolution of the saved image in dots/inches
  # bbox_inches - when set to 'tight' - does not allow the labels to be cropped
  plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')# this will will also return two dataset one being the original one and second one will be the one in
create_heat_map(data)

#data preprocessing steps
def get_type(data):
  numeric=[]
  categorical=[]
  for col in data.columns:
    if data[f'{col}'].dtypes == 'object':
      categorical.append(col)
    else:
      numeric.append(col)
  return {'numeric':numeric,'categorical':categorical}

# check for null values and deal with them
# this function will take the type of process as well for both numeric and categorical data
def treat_null_values(data,numeric_type:str='mean'):
  types=get_type(data)
  numeric=types['numeric']
  categorical=types['categorical']
  if numeric_type == 'mean':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].mean())
  elif numeric_type == 'mode':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].mode())
  elif numeric_type == 'median':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].median())
  elif numeric_type == 'frequent':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].nunique[0])
  elif numeric_type == 'drop':
    for col in numeric:
      data[f'{col}']=data[f'{col}'].dropnna(inplace=True)
  elif numeric_type == 'predictive_modeling':
    pass # create a seprate function for this
  elif numeric_type == 'impute':
    pass # create a seprate function for this as well
  
  for col in categorical:
    most_frequent_category=data[f'{col}'].mode()[0]
    data[f'{col}'].fillna(most_frequent_category,inplace=True)
  return data

  

def predictive_modeling():
  pass #do a detailed study as disadvantages for this model usually outweights advantages
def multiple_imputation():
  from fancyimpute import IterativeImputer as MICE
  data= pd.DataFrame(MICE().fit_transform(data))
  return data

def encode_data(data,multiclass:str='One_hot',binary_class:str='Label'): #this function takes three args one is the data 2nd is the type of encoding for multiclass data and third is the encoding for binary class data
  categorical=get_type(data)['categorical']
  multivariate=[]
  bivariate=[]
  for col in categorical:
    if data[f'{col}'].nunique()>2:
      multivariate.append(col)
    else:
      bivariate.append(col)
  
  if multiclass == 'One_hot':
    for col in multivariate:
      data=encode_and_bind(data,col)
  if binary_class == 'Label':
    for col in bivariate:
      data=label_encode(data,col)
  # add other sorting techniques as well in here
  return data

def encode_and_bind(original_dataframe, feature_to_encode):
    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])
    res = pd.concat([original_dataframe, dummies], axis=1)
    res.pop(feature_to_encode)
    return(res)

def label_encode(data,col):
  from sklearn.preprocessing import LabelEncoder
  encoder=LabelEncoder()
  data[col]=encoder.fit_transform(data[col])
  return data
  
# this function needs to be modified therefore add differenr sort of encoding techniques as well in this cll

def treat_outliers(data,feature:str,algo:str='IQR',z_threshold:int=3,add_feature=None,remove:bool=False,scatter:bool=False,feature_x=None,feature_y=None): # this function takes two arguments one is data and 2ns is the algorithm
  pass
  if scatter == True:
    px.scatter(data,x=feature_x,y=feature_y,hover_name='Country')
  else:
    if algo == 'IQR':
      outlier_index=Inter_quantile_range(data,feature)
    if algo == 'EEA': # elliptic envelope algo
      outlier_index=Elliptice_envelope_algo(data,feature,add_feature)
    if algo == 'ISF': # Isolate forest algo
      outlier_index=Isolate_forest_algo(data,feature)
    if algo == 'One_classSVM': # one class svm
      outlier_index=One_class_SVM(data,feature)
    if algo == 'LOF':
      outlier_index=Local_factor_outlier(data,feature)
    if algo == 'Z_score':
      outlier_index=Z_score_algo(data,feature,z_threshold)
    print(outlier_index)
    if remove == True:
      data=remove_outliers(outlier_index,data)
    
    return data
    


# in all these function display the index of outliers

def Local_factor_outlier(data,feature):
  import numpy as np 
  from sklearn.neighbors import LocalOutlierFactor
  X=data[[feature,data.columns.values[-1]]]
  lof = LocalOutlierFactor(n_neighbors=20, algorithm='auto',
                         metric='minkowski', contamination=0.04,
                         novelty=False, n_jobs=-1)
  pred = lof.fit_predict(X)
  outlier_index = np.where(pred==-1)
  return outlier_index

def One_class_SVM(data,feature): #this returns novelty I am guessing so fuck this method
  import numpy as np
  from sklearn.svm import OneClassSVM
  X=data[[feature,data.columns.values[-1]]]
  one_class_svm = OneClassSVM(kernel='rbf', degree=3, gamma='scale')
  new_data = np.array([[-4, 8.5]])# change these values as per your dataset
  one_class_svm.fit(X)
  pred = one_class_svm.predict(new_data)
  outlier_index = np.where(pred==-1)
  return outlier_index

def Isolate_forest_algo(data,feature):
  import numpy as np
  from sklearn.ensemble import IsolationForest
  from sklearn.decomposition import PCA
  from sklearn.preprocessing import StandardScaler
  X=data[[feature,data.columns.values[-1]]]
   # Returns 1 of inliers, -1 for outliers
  iforest = IsolationForest(n_estimators=100, max_samples='auto', 
                          contamination=0.05, max_features=1.0, 
                          bootstrap=False, n_jobs=-1, random_state=1)
  pred = iforest.fit_predict(X)
 # Extract outliers
  outlier_index = np.where(pred==-1)
  return outlier_index

def Elliptice_envelope_algo(data,feature,add_feature):
  import numpy as np
  from sklearn.covariance import EllipticEnvelope
  elpenv = EllipticEnvelope(contamination=0.025, 
                          random_state=1)
  X=data[[feature,add_feature]]
# Returns 1 of inliers, -1 for outliers
  pred = elpenv.fit_predict(X)

# Extract outliers
  outlier_index = np.where(pred==-1)
  return outlier_index

def Inter_quantile_range(data,feature):
  Q1 = np.percentile(data[f'{feature}'], 25, interpolation = 'midpoint') 
  Q2 = np.percentile(data[f'{feature}'], 50, interpolation = 'midpoint') 
  Q3 = np.percentile(data[f'{feature}'], 75, interpolation = 'midpoint') 
  IQR = Q3 - Q1 
  low_lim = Q1 - 1.5 * IQR
  up_lim = Q3 + 1.5 * IQR
  outlier_index=[]
  for val in data[f'{feature}']:
    if val > up_lim or val <low_lim:
      outlier_index.append(data.index[data[f'{feature}'] == val].values[0])
  return outlier_index
         
def Z_score_algo(data,feature,z_threshold):
  mean = np.mean(data[f'{feature}'])
  std = np.std(data[f'{feature}'])
  print('mean of the dataset is', mean)
  print('std. deviation is', std)
  outlier_index = []
  for val in data[f'{feature}']:
    z = (val-mean)/std
    if z > z_threshold:
      outlier_index.append(data.index[data[f'{feature}'] == val].values[0])
  return outlier_index
  


# this function will remove the outliers if removal is allowed
def remove_outliers(index,data):
  for row in index:
    data.drop([row])
    print(f'dropped value at index {row}')
  return data

      

# if remove is true remove outliers else if it is false just return the index of outliers
# this will function will be returning two datasets the original one and one in which all the outliers are removed

#since we are not filtering for any dependent variable in the clustering therefore feature is not require
def preprocess_data(data):
  data=treat_null_values(data)
  data=encode_data(data)
  return data

# call outliers and balance function independentlu whenever you like and PS Fuck Roopa

#dropping CUST_ID since it is not related to data and will just increase the dimension of the data
data.drop('CUST_ID',inplace=True,axis=1)

data=preprocess_data(data)

data.describe()

"""#Visualizing the processed data"""

data.columns

px.scatter(data, x="BALANCE", y="BALANCE_FREQUENCY", color="PURCHASES",
                 size='ONEOFF_PURCHASES', hover_data=['ONEOFF_PURCHASES'])

px.scatter(data, x="BALANCE", y="BALANCE_FREQUENCY", color="PURCHASES",
                 size='INSTALLMENTS_PURCHASES', hover_data=['INSTALLMENTS_PURCHASES'])

px.scatter(data, x="BALANCE", y="BALANCE_FREQUENCY", color="PURCHASES",
                 size='CASH_ADVANCE', hover_data=['CASH_ADVANCE'])

px.scatter(data, x="BALANCE", y="BALANCE_FREQUENCY", color="PURCHASES",
                 size='PURCHASES_FREQUENCY', hover_data=['PURCHASES_FREQUENCY'])

def reduced_Dataset(data,reduction:int,method:str='PCA'):
  if method == 'PCA':
    return PCA(data,reduction)
 
  elif method == 'Kernel':
    return KernelPCA(data,reduction)

#go with pca since kernel pca might take quite  a long time
  


def PCA(data,reduction):
  from sklearn.decomposition import PCA
  pca=PCA(n_components=reduction)#n_component=reduced dimension of dataset
  reduced_data=pca.fit_transform(data)
  return reduced_data

def KernelPCA(data,reduction):
  from sklearn.decomposition import KernelPCA
  pca=KernelPCA(n_components=reduction,kernel='rbf')
  reduced_data=pca.fit_transform(data)
  return reduced_data

#going with this data
data.shape

#k-mean clustering

#radom initializtion trap is taken care of automatically since the algoo implemented is k-means++
def kmeansClustering(data,nClusters):
  from sklearn.cluster import KMeans
  kmeans = KMeans(n_clusters = nClusters, init = 'k-means++', random_state = 42)
  y_kmeans = kmeans.fit_predict(data)
  return y_kmeans
  

def elbowMethod(data,min:int=2,max:int=20):
  from sklearn.cluster import KMeans
  wcss = []#within cluster sum of squares we use this matrice to define the effciecy of model foer k number of clusters
  for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(data)
    wcss.append(kmeans.inertia_)
  plt.plot(range(1, 11), wcss)
  plt.title('The Elbow Method')
  plt.xlabel('Number of clusters')
  plt.ylabel('WCSS')
  plt.show()

elbowMethod(data)

kmeansCluster=kmeansClustering(data,3)

#Hierarchial clustering
def HierarchialClustering(data,nClusters:int,affinity:str='euclidean',linkage:str='ward'):
  from sklearn.cluster import AgglomerativeClustering
  hc = AgglomerativeClustering(n_clusters = nClusters, affinity = affinity, linkage = linkage)
  y_hc = hc.fit_predict(data)
  return y_hc
def createDendogram(data):
  import scipy.cluster.hierarchy as sch
  dendrogram = sch.dendrogram(sch.linkage(data, method = 'ward'))
  # ward is the clusering techniique best suited for hierarchial clustering




  # ward stands for minimum variance
  plt.title('Dendrogram')
  plt.ylabel('Euclidean distances')
  plt.show()

createDendogram(data)

hierarchialCluster=HierarchialClustering(data,3)

#Mean shift clustering
#note this algorithm is quite clumsy for large datasets
#finding the optimal value of the bandwidth => this will be tackled by the second function
#this requires heavy computational resources so either reduce the dataset first or run it in jupyter notebook
def meanShiftClustering(data):
  from sklearn.cluster import MeanShift
  bandwidth = bandwidthDetermination(data)#this function will give you the otimal value for the bandwidth
  clustering = MeanShift(n_jobs=-1,bandwidth=bandwidth)
  cluster = clustering.fit_predict(data) #this algo will be clumsy for a large dataset
  #runnig time is O(n2)
  return cluster


#this function will return the bandwidth value that will be used in the above function
def bandwidthDetermination(data):
  from sklearn.cluster import estimate_bandwidth
  bandwidth = estimate_bandwidth(data)#for large datasets this will take some time
  return bandwidth

mean_shiftCluster=meanShiftClustering(data)

#minibatch kmeans algorithm
#we basically break the large dataset into small batches
# so that they can be allocated in memmory where as a large dataset can not be allocated into the memmory

def minBatchKmeans(data,nClusters:int,batchSize:int=10,max_iter:int=10):
  from sklearn.cluster import MiniBatchKMeans
  clustering = MiniBatchKMeans(n_clusters=nClusters,batch_size=batchSize,max_iter=max_iter,random_state=42)
  cluster=clustering.fit_predict(data)
  return cluster

min_batchCluster=minBatchKmeans(data,3)

#gaussian mixture models
#thia model can cluster non spherical data and it also
#it's always a good idea to reduce the dataset as it reduces the red flag variables in our dataset
def GMMClustering(data,nComponents:int=2):
  from sklearn.mixture import GaussianMixture
  clustering = GaussianMixture(n_components=nComponents,random_state=42)
  cluster = clustering.fit_predict(data)
  return cluster

gmmCluster=GMMClustering(data)

def birchAlgo(data,nClusters:int=2):
  from sklearn.cluster import Birch
  clustering = Birch(n_clusters = nClusters)#ooh ma good turu lobh
  cluster = clustering.fit_predict(data)
  return cluster

birchCluster=birchAlgo(data)

#in this section you can generate a new dataframe of dimension Nx3 to visualize the clusters
#the thing with  this is that some of the columns do not participate in cluster formation and some do since we applies dimensionality reduction
def generateDataframe(data,feature1,feature2):
  #generating new dataset
  feature1Data = data[feature1]
  feature2Data = data[feature2]
  cluster = data['Cluster']

  index=[]
  for i in range(0,data.shape[0]):
    index.append(i)

  columns = [feature1,feature2,'Cluster']


  data = []
  for feat1,feat2,label in zip(feature1Data,feature2Data,cluster):
    data.append([feat1,feat2,label])

  newData=pd.DataFrame(data=data,index=index,columns=columns)
  return newData

#generating some new datasets
data.columns

def generateCluster(data,feature1:str,feature2:str,model:str='kMeans'):
  new_data=data
  if 'Cluster' in new_data.columns:
    new_data.drop('Cluster',inplace=True,axis=1)
  if model == 'kMeans':
    new_data.insert(data.shape[1],'Cluster',kmeansCluster)
    new_data=generateDataframe(new_data,feature1,feature2)
    drawVisualizatioN(new_data,feature1,feature2)
  elif model == 'hierarchy':
    new_data.insert(data.shape[1],'Cluster',hierarchialCluster)
    new_data=generateDataframe(new_data,feature1,feature2)
    drawVisualizatioN(new_data,feature1,feature2)
  elif model == 'meanShift':
    new_data.insert(data.shape[1],'Cluster',mean_shiftCluster)
    new_data=generateDataframe(new_data,feature1,feature2)
    drawVisualizatioN(new_data,feature1,feature2)
  elif model == 'minBatch':
    new_data.insert(data.shape[1],'Cluster',min_batchCluster)
    new_data=generateDataframe(new_data,feature1,feature2)
    drawVisualizatioN(new_data,feature1,feature2)
  elif model == 'gmm':
    new_data.insert(data.shape[1],'Cluster',gmmCluster)
    new_data=generateDataframe(new_data,feature1,feature2)
    drawVisualizatioN(new_data,feature1,feature2)
  elif model == 'birch':
    new_data.insert(data.shape[1],'Cluster',birchCluster)
    new_data=generateDataframe(new_data,feature1,feature2)
    drawVisualizatioN(new_data,feature1,feature2)

colors = ['red','blue','magenta','green','cadetblue','chartreuse',
          'chocolate','coral','cornflowerblue','cornsilk','crimson',
          'cyan','darkblue','darkcyan','darkgoldenrod','darkgray'
          ,'dodgerblue','firebrick','floralwhite','forestgreen',
          'fuchsia','gainsboro','ghostwhite','gold','goldenrod','gray','greenyellow',
          'honeydew','hotpink','linen',
          'maroon','mediumaquamarine','olivedrab','tan','teal','thistle','tomato','turquoise','violet',
          'wheat','white','whitesmoke','yellowgreen','antiquewhite','aqua','aquamarine',
          'azure','beige','bisque','black','blanchedalmond',
          'blueviolet','brown','burlywood','indianred',
          'mintcream','mistyrose','moccasin','navajowhite','navy','oldlace','olive','darkmagenta','darkolivegreen',
          'darkorange','darkorchid','indigo','ivory','khaki','lavender','lavenderblush',
          'lawngreen','lemonchiffon','lightblue','lightcoral','darkgreen','darkturquoise','darkviolet','deeppink'
          ,'deepskyblue','dimgray','lightcyan','lightgoldenrodyellow',
          'lightgreen','lightgray','lightpink','lightsalmon','lightseagreen','lightskyblue',
          'lightslategray','lightsteelblue','lightyellow','lime','limegreen',
          'orange','orangered','orchid','palegoldenrod','palegreen','paleturquoise','palevioletred',
          'papayawhip','peachpuff','peru','pink','plum','powderblue','purple','aliceblue','rosybrown','royalblue',
          'saddlebrown','salmon','sandybrown','seagreen','seashell','sienna','silver','skyblue','slateblue',
          'slategray','snow','springgreen','steelblue','yellow',]
def drawVisualizatioN(data,xLabel:str,yLabel:str):
  import random
  classes=data['Cluster'].unique()
  for i in classes:
    plt.scatter(data[data['Cluster'] == i][data.columns[0]],data[data['Cluster'] == i][data.columns[1]],s=100,c=random.choice(colors),label=f'Cluster{i+1}')
  
  plt.title('Cluster of customers based on theri credit card score')
  plt.xlabel(xLabel)
  plt.ylabel(yLabel)
  plt.legend()
  plt.show()

"""#Generating many visualizations of the cluster using the above function"""

data.columns

#try different combinations of the features you want to study to visualize the cluster

generateCluster(data,'PURCHASES','PAYMENTS')

generateCluster(data,'PURCHASES','CREDIT_LIMIT')

generateCluster(data,'PURCHASES','CASH_ADVANCE_TRX','hierarchy')

generateCluster(data,'PURCHASES','CASH_ADVANCE_FREQUENCY','hierarchy')

generateCluster(data,'PURCHASES','ONEOFF_PURCHASES_FREQUENCY','hierarchy')

generateCluster(data,'PURCHASES','PURCHASES_FREQUENCY','hierarchy')

generateCluster(data,'PURCHASES','CASH_ADVANCE','birch')

generateCluster(data,'PURCHASES','INSTALLMENTS_PURCHASES','hierarchy')

generateCluster(data,'PURCHASES','ONEOFF_PURCHASES','hierarchy')





generateCluster(data,'PURCHASES','CREDIT_LIMIT')

generateCluster(data,'PURCHASES','CREDIT_LIMIT','hierarchy')

generateCluster(data,'PURCHASES','CREDIT_LIMIT','minBatch')

generateCluster(data,'PURCHASES','CREDIT_LIMIT','gmm')

generateCluster(data,'PURCHASES','CREDIT_LIMIT','birch')

"""#Evaluating the models """