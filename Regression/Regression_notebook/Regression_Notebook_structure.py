# -*- coding: utf-8 -*-
"""Regression Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QnzE24kakTvV05pcrDHwdK8VNajbArEm

# **This notebook contains all the basic regression tools I use for my work.This will be modified with time and it's repo and previous copies are available in my github account**
"""

#import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
sns.set(style="darkgrid")

#generate your dataset
def generate_data(location:str,sample_number=10):
  data=pd.read_csv(location)
  head=data.head()
  tail=data.tail()
  sample=data.sample(sample_number)
  description=data.describe()
  columns=data.columns
  info=data.info()
  shape=data.shape
  size=data.size
  return {'data':data,'head':head,'tail':tail,'sample':sample,'description':description,'columns':columns,'info':info,
          'shape':shape,'size':size}

data_set=generate_data('/content/Life Expectancy Data.csv')

# check for null values and deal with them
# this function will take the type of process as well for both numeric and categorical data
def treat_null_values(numeric_type:str='mean',categorical_type:str='frequent'):
  pass

def encode_data(data,multiclass:str='One_hot',binary_class:str='Label'): #this function takes three args one is the data 2nd is the type of encoding for multiclass data and third is the encoding for binary class data
  pass

def treat_outliers(data,algo:str='IQR'): # this function takes two arguments one is data and 2ns is the algorithm
  pass
# this will function will be returning two datasets the original one and one in which all the outliers are removed

def balance_data(data):
  pass

# this function will perform all the data preprocessing steps
def preprocess_data(data):
  data=treat_null_values()
  data=encode_data()
  data=treat_outliers()
  data=balance_data()
  return data

#this function will check for collinearity and treat it as well
def check_coolinearity(data):
  pass
# this will will also return two dataset one being the original one and second one will be the one in
# which all the coolinear features will be treated:

def feature_engineering(data):
  # in this function do all the feature enginerring stuff you want to do

#visualize your data using this function
#in all the functions pass folparameters
'''
1)type of plot
2)feature that will be used in x_axis
3)featur that will be used in y_axis
4)parametres that will be used while drawing the plot
'''
def matplot_lib_visualize(type:str,x:str,y:str,*params): 
  pass
def seaborn_visualize(type:str,x:str,y:str,*params):
  pass
def plotly_visualize(type:str,x:str,y:str,*params):
  pass

"""#Prepare a descriptive table in here on how to use this function"""

def split_datad(data,test_data_percentage=0.25,random_state=42):
  pass
# this function takes three args one being the data and other the percent of test data and the random_state
# this function will return 4 args x_train,x_test,y_train,y_test
# add shape and size as well for all the params
# in this format 
#{'x_train':{"data":data,"shape":data.shape,"size":data.size}}

def scale_data(x_train,x_test,type:str='Standard'):
  pass
 # this function takes three args x_train,x_test and the type of scaling that should be applied

def prepate_model(x_train,y_train):
  pass
  #prepare all the models
# this function will return a dictionary of models

def make_predictios(models)

def concate_test_pred(predictions,y_test):
  pass
  #this function will return  a dictionary containing the concatenated list of y_test and y_pred for each model

def visualize_models(y_test,models):
  pass
#models here is a dict of models using which models will be evaluated

def r2_score_models(models,x_train,y_train):
  pass
# in this function prepare a dictionary for all the models and the dict will include the mean r2 and the r2 score list

def model_evaluation(models,x_train,y_train):
  pass
# this function will return a dataframe including the different matrices on basis of which a model should be evaluted
# foe each model
# use k-fold cross validation here

def study_model(models):
  pass
# use bootstrapping here two study the deviation and distrubution of params of each trained model

def hypertune_model(model,*param_grid,cv=10,n_jobs=-1):
  pass
# this function will accept a model and all the configurable hyperparametres and will return the tuned model

#prepare a dictionary that will containt the hyperparametes for all the models and can be easily accessed

def reduce_dimension(algo:str='PCA',kernel:str='rbf'):
  pass
  # this function will return a new dataset with reduced dimensions in case you want to reduce dimension