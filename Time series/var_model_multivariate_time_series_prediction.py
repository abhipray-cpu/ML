# -*- coding: utf-8 -*-
"""VAR model multivariate Time series prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w7xM1YiZBY1qYJrZcdSSfJn14uKSJzDd

#This notebook is for analyzing the multivariate time series and making predictions upon it
"""

!pip install statsmodels --upgrade

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

filepath = 'https://raw.githubusercontent.com/selva86/datasets/master/Raotbl6.csv'
varData = pd.read_csv(filepath, parse_dates=['date'], index_col='date')
print(varData.shape)  # (123, 8)
varData.tail()

"""#The dataset has following 8 quaterly time series

1. rgnp  : Real GNP.
2. pgnp  : Potential real GNP.
3. ulc   : Unit labor cost.
4. gdfco : Fixed weight deflator for personal consumption expenditure excluding food and energy.
5. gdf   : Fixed weight GNP deflator.
6. gdfim : Fixed weight import deflator.
7. gdfcf : Fixed weight deflator for food in personal consumption expenditure.
8. gdfce : Fixed weight deflator for energy in personal consumption expenditure.
"""

def visualuze_multivariate_time_series(data,rows:int=4,cols:int=2):
  fig, axes = plt.subplots(nrows=rows, ncols=cols, dpi=120, figsize=(10,6))
  # i will simply give the column count/index
  for i, ax in enumerate(axes.flatten()):
    df = data[data.columns[i]]
    ax.plot(df, color='red', linewidth=1)
    fig.tight_layout(pad=3.0)#this is for providing proper spacing between the plots
    # Decorations
    ax.set_title(data.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)
visualuze_multivariate_time_series(varData)

"""#Testing Causation using Granger’s Causality Test

The basis behind Vector AutoRegression is that each of the time series in the system influences each other. That is, you can predict the series with past values of itself along with other series in the system.

Using Granger’s Causality Test, it’s possible to test this relationship before even building the model.

So what does Granger’s Causality really test?

Granger’s causality tests the null hypothesis that the coefficients of past values in the regression equation is zero.

In simpler terms, the past values of time series (X) do not cause the other series (Y). So, if the p-value obtained from the test is lesser than the significance level of 0.05, then, you can safely reject the null hypothesis.


"""

def grangers_causation_matrix(data, variables,maxlag:int=12, test:str='ssr_chi2test', verbose:bool=False):    
    """Check Granger Causality of all possible combinations of the Time series.
    The rows are the response variable, columns are predictors. The values in the table 
    are the P-Values. P-Values lesser than the significance level (0.05), implies 
    the Null Hypothesis that the coefficients of the corresponding past values is 
    zero, that is, the X does not cause Y can be rejected.

    data      : pandas dataframe containing the time series variables
    variables : list containing names of the time series variables.
    """
    from statsmodels.tsa.stattools import grangercausalitytests
    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)
    for c in df.columns:
        for r in df.index:
            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)
            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]
            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')
            min_p_value = np.min(p_values)
            df.loc[r, c] = min_p_value
    df.columns = [var + '_x' for var in variables]
    df.index = [var + '_y' for var in variables]
    return df


#how to read this table?
'''
If a given p-value is < significance level (0.05), 
then, the corresponding X time series (column) causes/effects the Y time series (row).
'''


grangers_causation_matrix(varData, variables = varData.columns)

"""#Cointegration test✈

**Cointegration test helps to establish the presence of a statistically significant connection between two or more time series.**

But, what does Cointegration mean?

To understand that, you first need to know what is ‘order of integration’ (d).

**Order of integration(d) is nothing but the number of differencing required to make a non-stationary time series stationary.**

**Now, when you have two or more time series, and there exists a linear combination of them that has an order of integration (d) less than that of the individual series, then the collection of series is said to be cointegrated.**

Ok?

When two or more time series are cointegrated, it means they have a long run, statistically significant relationship.

This is the basic premise on which Vector Autoregression(VAR) models is based on. So, it’s fairly common to implement the cointegration test before starting to build VAR models.
"""

def cointegration_test(df, alpha=0.05):
  from statsmodels.tsa.vector_ar.vecm import coint_johansen 
  """Perform Johanson's Cointegration Test and Report Summary"""
  out = coint_johansen(df,-1,5)
  d = {'0.90':0, '0.95':1, '0.99':2}
  traces = out.lr1
  cvts = out.cvt[:, d[str(1-alpha)]]
  def adjust(val, length= 6): return str(val).ljust(length)
  # Summary
  print('Name   ::  Test Stat > C(95%)    =>   Signif  \n', '--'*20)
  for col, trace, cvt in zip(df.columns, traces, cvts):
    print(adjust(col), ':: ', adjust(round(trace,2), 9), ">", adjust(cvt, 8), ' =>  ' , trace > cvt)

cointegration_test(varData)

#splitting the dataset into train and test set
nobs = 4 #this is the numbe of elements that will be contained in the test set
df_train, df_test = varData[0:-nobs], varData[-nobs:]

# Check size
print(df_train.shape)  # (119, 8)
print(df_test.shape)  # (4, 8)

#checking for stationarity and making the time series stationary
#implementing ADFuller test with significance/threshold value of 0.5
def adfuller_test(series, signif=0.05, name='', verbose=False):
  from statsmodels.tsa.stattools import adfuller
  """Perform ADFuller to test for Stationarity of given series and print report"""
  r = adfuller(series, autolag='AIC')
  output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}
  p_value = output['pvalue'] 
  def adjust(val, length= 6): return str(val).ljust(length)
  # Print Summary
  print(f'    Augmented Dickey-Fuller Test on "{name}"', "\n   ", '-'*47)
  print(f' Null Hypothesis: Data has unit root. Non-Stationary.')
  print(f' Significance Level    = {signif}')
  print(f' Test Statistic        = {output["test_statistic"]}')
  print(f' No. Lags Chosen       = {output["n_lags"]}')
  for key,val in r[4].items():
    print(f' Critical value {adjust(key)} = {round(val, 3)}')
  
  if p_value <= signif:
    print(f" => P-Value = {p_value}. Rejecting Null Hypothesis.")
    print(f" => Series is Stationary.")
  else:
    print(f" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")
    print(f" => Series is Non-Stationary.")

#implementing ADFuller test on each column
for name, column in df_train.iteritems():
    adfuller_test(column, name=column.name)
    print('\n')

#since none of the time series is stationaty therefore differencing all of them
df_differenced = df_train.diff().dropna()
#AD Fuller test on each of the tim series
for name, column in df_differenced.iteritems():
    adfuller_test(column, name=column.name)
    print('\n')

#since some of the time series are still non stationary therfore again differencing them
df_differenced = df_differenced.diff().dropna()
# ADF Test on each column of 2nd Differences Dataframe
for name, column in df_differenced.iteritems():
    adfuller_test(column, name=column.name)
    print('\n')

#All the time series are stationary now we can proceed
#How to select the right order P of the var model
def check_lag(data):
  from statsmodels.tsa.api import VAR
  model = VAR(data)
  for i in [1,2,3,4,5,6,7,8,9]:
    result = model.fit(i)
    print('Lag Order =', i)
    print('AIC : ', result.aic)
    print('BIC : ', result.bic)
    print('FPE : ', result.fpe)
    print('HQIC: ', result.hqic, '\n')

# we select the model and one of the criteria is BIC the value in inversely proportional to the performance of the model
check_lag(df_differenced)

#or we can use this function
def check_lag_auto(data,maxlags:int=12):
  from statsmodels.tsa.api import VAR
  model = VAR(df_differenced)
  x = model.select_order(maxlags=12)
  display(x.summary())

check_lag_auto(df_differenced)

#selecting the lag 4 since it is giving the best results
def train_model(data,lag:int):
  from statsmodels.tsa.api import VAR
  model = VAR(data)
  model_fitted = model.fit(lag)
  return model_fitted

var_model = train_model(df_differenced,4)
var_model.summary()

"""#Check for Serial Correlation of Residuals (Errors) using Durbin Watson Statistic

# Serial correlation of residuals is used to check if there is any leftover pattern in the residuals (errors).

What does this mean to us?

If there is any correlation left in the residuals, then, there is some pattern in the time series that is still left to be explained by the model. In that case, the typical course of action is to either increase the order of the model or induce more predictors into the system or look for a different algorithm to model the time series.

So, checking for serial correlation is to ensure that the model is sufficiently able to explain the variances and patterns in the time series.

Alright, coming back to topic.
"""

'''
The value of this statistic can vary between 0 and 4.
 The closer it is to the value 2, then there is no 
 significant serial correlation. 
 The closer to 0, there is a positive serial correlation,
  and the closer it is to 4 implies negative serial correlation.
'''
from statsmodels.stats.stattools import durbin_watson
out = durbin_watson(var_model.resid)

for col, val in zip(varData.columns, out):
    print(col, ':', round(val, 2))

#forecasting using varModel
# Get the lag order
lag_order = var_model.k_ar
print(lag_order)  #> 4

# Input data for forecasting
forecast_input = df_differenced.values[-lag_order:]
forecast_input

#forecast
fc = var_model.forecast(y=forecast_input, steps=nobs)
df_forecast = pd.DataFrame(fc, index=varData.index[-nobs:], columns=varData.columns + '_2d')
df_forecast

#invert the transformation to get the real data
def invert_transformation(df_train, df_forecast, second_diff=False):
    """Revert back the differencing to get the forecast to original scale."""
    df_fc = df_forecast.copy()
    columns = df_train.columns
    for col in columns:        
        # Roll back 2nd Diff
        if second_diff:
            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()
        # Roll back 1st Diff
        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()
    return df_fc

df_results = invert_transformation(df_train, df_forecast, second_diff=True)        
df_results.loc[:, ['rgnp_forecast', 'pgnp_forecast', 'ulc_forecast', 'gdfco_forecast',
                   'gdf_forecast', 'gdfim_forecast', 'gdfcf_forecast', 'gdfce_forecast']]

#Plotting forecast vs actual data
fig, axes = plt.subplots(nrows=int(len(varData.columns)/2), ncols=2, dpi=150, figsize=(15,20))
for i, (col,ax) in enumerate(zip(varData.columns, axes.flatten())):
    df_results[col+'_forecast'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)
    df_test[col][-nobs:].plot(legend=True, ax=ax);
    fig.tight_layout(pad=3.0)
    ax.set_title(col + ": Forecast vs Actuals")
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)

#Evaluating the model 
def forecast_accuracy(forecast, actual):
  from statsmodels.tsa.stattools import acf
  mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE
  me = np.mean(forecast - actual)             # ME
  mae = np.mean(np.abs(forecast - actual))    # MAE
  mpe = np.mean((forecast - actual)/actual)   # MPE
  rmse = np.mean((forecast - actual)**2)**.5  # RMSE
  corr = np.corrcoef(forecast, actual)[0,1]   # corr
  mins = np.amin(np.hstack([forecast[:,None], 
                              actual[:,None]]), axis=1)
  maxs = np.amax(np.hstack([forecast[:,None], 
                              actual[:,None]]), axis=1)
  minmax = 1 - np.mean(mins/maxs)             # minmax
  return({'mape':mape, 'me':me, 'mae': mae, 
            'mpe': mpe, 'rmse':rmse, 'corr':corr, 'minmax':minmax})
  
#or you can loop through all the columns in the dataset and calculate for them
print('Forecast Accuracy of: rgnp')
accuracy_prod = forecast_accuracy(df_results['rgnp_forecast'].values, df_test['rgnp'])
for k, v in accuracy_prod.items():
    print(k, ': ', round(v,4))
print('\nForecast Accuracy of: pgnp')
accuracy_prod = forecast_accuracy(df_results['pgnp_forecast'].values, df_test['pgnp'])
for k, v in accuracy_prod.items():
    print(k, ': ', round(v,4))

print('\nForecast Accuracy of: ulc')
accuracy_prod = forecast_accuracy(df_results['ulc_forecast'].values, df_test['ulc'])
for k, v in accuracy_prod.items():
    print(k, ': ', round(v,4))

print('\nForecast Accuracy of: gdfco')
accuracy_prod = forecast_accuracy(df_results['gdfco_forecast'].values, df_test['gdfco'])
for k, v in accuracy_prod.items():
    print(k, ': ', round(v,4))

print('\nForecast Accuracy of: gdf')
accuracy_prod = forecast_accuracy(df_results['gdf_forecast'].values, df_test['gdf'])
for k, v in accuracy_prod.items():
    print(k, ': ', round(v,4))

print('\nForecast Accuracy of: gdfim')
accuracy_prod = forecast_accuracy(df_results['gdfim_forecast'].values, df_test['gdfim'])
for k, v in accuracy_prod.items():
    print(k, ': ', round(v,4))

print('\nForecast Accuracy of: gdfcf')
accuracy_prod = forecast_accuracy(df_results['gdfcf_forecast'].values, df_test['gdfcf'])
for k, v in accuracy_prod.items():
    print(k, ': ', round(v,4))

print('\nForecast Accuracy of: gdfce')
accuracy_prod = forecast_accuracy(df_results['gdfce_forecast'].values, df_test['gdfce'])
for k, v in accuracy_prod.items():
    print(k, ': ', round(v,4))